{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library & Constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Options\n",
    "\n",
    "XGB_FEATER_SELECTION = False\n",
    "BINARY_CLASSIFICATION = True\n",
    "\n",
    "# Paths\n",
    "\n",
    "DATASET_DIR = './data'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "\n",
    "# Datsets Info\n",
    "\n",
    "BENIGN_LABEL = ['Benign']\n",
    "MALICIUS_LABELS = {\n",
    "    \"Web_Attack\": [\"Web_XSS\", \"Web_SQL_Injection\", \"Web_Brute_Force\"],\n",
    "    \"Dos_Attack\": [\"DoS_Slowhttptest\", \"DoS_GoldenEye\", \"DDoS LOIT\", \"DoS_Hulk\", \"DoS_Slowloris\"],\n",
    "    \"Network_Attack\": [\"Port_Scan\", \"Heartbleed\"],\n",
    "    \"Botnet\": [\"Botnet_ARES\"],\n",
    "    \"Service_Attack\": [\"SSH-Patator\", \"FTP-Patator\"]\n",
    "}\n",
    "LABELS = BENIGN_LABEL + list(MALICIUS_LABELS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Prima di procedere con la modellazione del dataset e alla creazione del modello, procediamo con il capire come i dati sono strutturati, cosa rappresentano e quali modifiche rapportare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "def load_data(datasets_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all datasets from a directory into a single DataFrame\n",
    "\n",
    "    :param str datasets_dir: directory containing the datasets\n",
    "    :return: pd.DataFrame containing all datasets\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            data.append(pd.read_csv(os.path.join(datasets_dir, file)))\n",
    "\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "\n",
    "dataset = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo l’analisi osservando la quantità di dati disponibili nel nostro dataset, BCCC-CIC-IDS2017, che comprende un totale impressionante di 2.438.052 record. Questi dati sono stati generati utilizzando NTLFlowLyzer, in contrasto con la versione precedente del dataset (CIC-IDS2017), che aveva invece adottato lo strumento CICFlowMeter per l'estrazione.\n",
    "\n",
    "Per approfondire e comprendere meglio i dati a disposizione, esploriamoli graficamente per analizzarne la distribuzione e identificarne eventuali pattern significativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "\n",
    "def plot_attack_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a bar chart to visualize the distribution of attack labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        attack_data = dataset[~dataset['label'].isin(BENIGN_LABEL)]['label'].value_counts()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(26, 10))\n",
    "        ax.bar(attack_data.index, attack_data.values, width=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a pie chart to visualize the distribution of 'Benign' and 'Attack' labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        data = {\n",
    "            'Benign': dataset['label'].isin(BENIGN_LABEL).sum(),\n",
    "            'Malicius': dataset[~dataset['label'].isin(BENIGN_LABEL)]['label'].value_counts().sum()\n",
    "        }\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie(data.values(), labels=data.keys(), autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#D4FCC3'])\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "plot_distribution(dataset)\n",
    "plot_attack_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi dei dati emergono due osservazioni principali:\n",
    "1. Il traffico normale è significativamente più rappresentato rispetto al traffico malevolo. Questo squilibrio può essere affrontato in modo intuitivo attraverso l’applicazione di tecniche di __undersampling__ sui dati relativi al traffico normale.\n",
    "2. La distribuzione dei dati degli attacchi non è uniforme.\n",
    "\n",
    "Concentrandoci sul secondo punto, l’alto sbilanciamento dei dati può influire negativamente sulle prestazioni del modello, aumentando il rischio di errori di predizione, soprattutto per le classi meno rappresentate. Per mitigare questo problema, è possibile adottare diversi approcci, ognuno con vantaggi e svantaggi:\n",
    "\n",
    "- Escludere attacchi con pochi dati\n",
    "- Applicare tecniche di undersampling e oversampling(undersampling).\n",
    "- Focalizzarsi su un singolo tipo di attacco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Per semplicità, procediamo con l'under sampling ed escludiamo il problema della distribuzione degli attacchi etichettando i dati solo con \"Benign\" e \"Malicious\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "from typing import List\n",
    "\n",
    "def clean_data(dataset: pd.DataFrame, columns_to_remove: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to clean.\n",
    "    :param List[str] columns_to_remove: The columns to remove from the dataset.\n",
    "    :return: The cleaned dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    float_cols = dataset.select_dtypes(include=['float']).columns\n",
    "    dataset[float_cols] = dataset[float_cols].round(4)\n",
    "\n",
    "    dataset = dataset.drop(columns=columns_to_remove)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "columns_to_remove = [\"flow_id\", \"src_ip\", \"dst_ip\", \"src_port\", \"timestamp\"]\n",
    "dataset = clean_data(dataset, columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labeling\n",
    "\n",
    "def binary_labeling(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'label' column in the given dataset to binary labels.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to convert.\n",
    "    :return: The dataset with binary labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['label'] = dataset['label'].apply(lambda x: 'Benign' if x in BENIGN_LABEL else 'Malicius')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def multiclass_labeling(dataset: pd.DataFrame, mapping_label: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'label' column in the given dataset to multiclass labels.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to convert.\n",
    "    :param dict mapping_label: The mapping of labels to convert to.\n",
    "    :return: The dataset with multiclass labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['label'] = dataset['label'].map(mapping_label)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if BINARY_CLASSIFICATION:\n",
    "    dataset = binary_labeling(dataset)\n",
    "else:\n",
    "    group_map = {attack: group for group, attacks in MALICIUS_LABELS.items() for attack in attacks}\n",
    "    group_map.update({BENIGN_LABEL[0]: BENIGN_LABEL[0]})\n",
    "    dataset = multiclass_labeling(dataset, group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Balancing\n",
    "\n",
    "def balance_data(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balances the given dataset by upsampling the minority class.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to balance.\n",
    "    :return: The balanced dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    benign_data = dataset[dataset['label'].isin(BENIGN_LABEL)]\n",
    "    malicious_data = dataset[~dataset['label'].isin(BENIGN_LABEL)]\n",
    "\n",
    "    malicious_data = resample(malicious_data, replace=True, n_samples=len(benign_data), random_state=42)\n",
    "    dataset = pd.concat([benign_data, malicious_data])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = balance_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Plotting after Data Cleaning and Unbalancing\n",
    "\n",
    "plot_attack_distribution(dataset)\n",
    "plot_distribution(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Features and Labels\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def extract_feature_and_target(dataset: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Extracts the feature and target columns from the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to extract the feature and target columns from.\n",
    "    :return: The feature and target columns.\n",
    "    \"\"\"\n",
    "\n",
    "    return dataset.drop(columns=['label']), dataset['label']\n",
    "\n",
    "X, y = extract_feature_and_target(dataset)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encoding(X: pd.DataFrame, y: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies one-hot encoding to all columns of type \"object\" in a pandas DataFrame.\n",
    "\n",
    "    :param pd.DataFrame df: The DataFrame to apply one-hot encoding to.\n",
    "    :return: The DataFrame with one-hot encoding applied.\n",
    "    \"\"\"\n",
    "\n",
    "    object_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    encoded_array = encoder.fit_transform(X[object_columns])\n",
    "    encoded_columns = encoder.get_feature_names_out(object_columns)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoded_columns)\n",
    "    X = pd.concat(\n",
    "        [X.drop(columns=object_columns).reset_index(drop=True), encoded_df.reset_index(drop=True)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = one_hot_encoding(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from typing import Tuple\n",
    "\n",
    "def normalize_data(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the given dataset using Min-Max scaling.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to normalize.\n",
    "    :return: The normalized dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_array = scaler.fit_transform(dataset)\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_array, columns=dataset.columns, index=dataset.index)\n",
    "    return normalized_df\n",
    "\n",
    "X = normalize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def select_features(dataset: pd.DataFrame, target: pd.Series, threshold: float = 0.01, using_xgb: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the most important features from the given dataset using an XGBoost classifier.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to select features from.\n",
    "    :param pd.Series target: The target column.\n",
    "    :param float threshold: The threshold to select features.\n",
    "    :param bool xgb: Whether to use an XGBoost classifier or RandomForest.\n",
    "    :return: The selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    if using_xgb:\n",
    "        clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(dataset, target)\n",
    "    \n",
    "    sfm = SelectFromModel(clf, threshold=threshold, prefit=True)    \n",
    "    \n",
    "    return dataset.columns[sfm.get_support()]\n",
    "\n",
    "\n",
    "features = select_features(X, y, using_xgb=XGB_FEATER_SELECTION)\n",
    "\n",
    "print(f\"Selected features with threshold >= 0.01:\")\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "\n",
    "X = X[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "Procederemo con lo sviluppo del modello facendo riferimento progetto github [_\"Efficient-CNN-BiLSTMs\"_](https://github.com/jayxsinha/Efficient-CNN-BiLSTM-for-Network-IDS/) per la sua struttura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Desing\n",
    "\n",
    "I layer di questo modello sono i seguenti:\n",
    "- CNN: Utile per apprendere le relazioni più nascoste tra le feature\n",
    "- 2 BiLSTMs: Per poter capire come le feature hanno effetto su lungo raggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Bidirectional, LSTM, Reshape, Dropout, Dense, Activation\n",
    "\n",
    "def build_model(input_shape=(122, 1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1D Convolutional layer\n",
    "    model.add(Convolution1D(64, kernel_size=122, padding=\"same\", activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    \n",
    "    # Reshape layer\n",
    "    model.add(Reshape((128, 1), input_shape=(128,)))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Second Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "    \n",
    "    # Dropout layer\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Dense output layer with sigmoid activation\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape=(len(features), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_model(model, X_train, y_train, batch_size=32, epochs=10, validation_data=None):\n",
    "    \"\"\"\n",
    "    Trains the given model.\n",
    "\n",
    "    :param keras.models.Sequential model: The model to train.\n",
    "    :param np.ndarray X_train: The training features.\n",
    "    :param np.ndarray y_train: The training target.\n",
    "    :param int batch_size: The batch size for training.\n",
    "    :param int epochs: The number of epochs for training.\n",
    "    :param Tuple[np.ndarray, np.ndarray] validation_data: The validation features and target.\n",
    "    :return: The trained model and training history.\n",
    "    \"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_data\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "model, history = train_model(model, X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Evaluation\n",
    "\n",
    "def plot_training_metrics(history) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics from the history object.\n",
    "\n",
    "    :param history: The training history object.\n",
    "    :return: Matplotlib figure of the training metrics.\n",
    "    \"\"\"\n",
    "    print(\"Generating training metrics plot...\")\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    train_acc = history.history.get('accuracy', None)\n",
    "    val_acc = history.history.get('val_accuracy', None)\n",
    "    if train_acc and val_acc:\n",
    "        ax[0].plot(train_acc, label='Training Accuracy', marker='o')\n",
    "        ax[0].plot(val_acc, label='Validation Accuracy', marker='x')\n",
    "        ax[0].set_title('Training and Validation Accuracy per Epoch')\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Accuracy')\n",
    "        ax[0].legend()\n",
    "        ax[0].grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history.get('val_loss', None)\n",
    "    ax[1].plot(train_loss, label='Training Loss', marker='o')\n",
    "    if val_loss:\n",
    "        ax[1].plot(val_loss, label='Validation Loss', marker='x')\n",
    "    ax[1].set_title('Training and Validation Loss per Epoch')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_history = plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_evaluation_metrics(y_test, y_pred, y_pred_probs) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics including confusion matrix and ROC curve.\n",
    "\n",
    "    :param y_test: True labels.\n",
    "    :param y_pred: Predicted labels.\n",
    "    :param y_pred_probs: Predicted probabilities.\n",
    "    :return: Matplotlib figure of the evaluation metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    labels = [\"Benign\", \"Malicious\"]\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"Original\")\n",
    "    axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    axes[1].plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", color=\"darkorange\")\n",
    "    axes[1].plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "    axes[1].set_xlabel(\"False Positive Rate\")\n",
    "    axes[1].set_ylabel(\"True Positive Rate\")\n",
    "    axes[1].set_title(\"Receiver Operating Characteristic\")\n",
    "    axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "def evaluate_model(model, X_test, y_test) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Evaluates the model and generates plots for evaluation metrics.\n",
    "\n",
    "    :param model: Trained model.\n",
    "    :param X_test: Test features.\n",
    "    :param y_test: Test labels.\n",
    "    :return: Matplotlib figure containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    y_pred_probs = model.predict(X_test).ravel()\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    return plot_evaluation_metrics(y_test, y_pred, y_pred_probs)\n",
    "\n",
    "\n",
    "plot_cm_and_roc = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "def save_model(model, path: str = MODEL_DIR, *plots: plt.Figure):\n",
    "    \"\"\"\n",
    "    Saves the given model to the specified path.\n",
    "\n",
    "    :param keras.models.Sequential model: The model to save.\n",
    "    :param plt.Figure plot: The plot to save.\n",
    "    :param str path: The path to save the model to.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    version = len(os.listdir(path)) + 1\n",
    "    model_dir = os.path.join(path, f\"v{version}\")\n",
    "\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "    model.save(os.path.join(model_dir, \"model.keras\"))\n",
    "    for i, plot in enumerate(plots):\n",
    "        plot.savefig(os.path.join(model_dir, f\"plot_{i}.png\"))\n",
    "\n",
    "\n",
    "save_model(model, plot_cm_and_roc, plot_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
