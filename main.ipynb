{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library & Constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Paths\n",
    "\n",
    "DATASET_DIR = './data'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "\n",
    "# Datsets Info\n",
    "\n",
    "BENIGN_LABEL = ['Benign']\n",
    "MALICIUS_LABELS = [\n",
    "    'Web_XSS', 'Heartbleed', 'Web_SQL_Injection', 'DoS_Slowhttptest',\n",
    "    'DoS_GoldenEye', 'Port_Scan', 'DDoS_LOIT', 'Botnet_ARES', 'Web_Brute_Force',\n",
    "    'SSH-Patator', 'FTP-Patator', 'DoS_Hulk', 'DoS_Slowloris'\n",
    "]\n",
    "LABELS = BENIGN_LABEL + MALICIUS_LABELS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Prima di procedere con la modellazione del dataset e alla creazione del modello, procediamo con il capire come i dati sono strutturati, cosa rappresentano e quali modifiche rapportare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "def load_data(datasets_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all datasets from a directory into a single DataFrame\n",
    "\n",
    "    :param str datasets_dir: directory containing the datasets\n",
    "    :return: pd.DataFrame containing all datasets\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            data.append(pd.read_csv(os.path.join(datasets_dir, file)))\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "\n",
    "dataset = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo l’analisi osservando la quantità di dati disponibili nel nostro dataset, BCCC-CIC-IDS2017, che comprende un totale impressionante di 2.438.052 record. Questi dati sono stati generati utilizzando NTLFlowLyzer, in contrasto con la versione precedente del dataset (CIC-IDS2017), che aveva invece adottato lo strumento CICFlowMeter per l'estrazione.\n",
    "\n",
    "Per approfondire e comprendere meglio i dati a disposizione, esploriamoli graficamente per analizzarne la distribuzione e identificarne eventuali pattern significativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "\n",
    "def plot_attack_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a bar chart to visualize the distribution of attack labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        attack_data = dataset[dataset['label'].isin(MALICIUS_LABELS)]['label'].value_counts()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(26, 10))\n",
    "        ax.bar(attack_data.index, attack_data.values, width=0.3)\n",
    "    \n",
    "\n",
    "def plot_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a pie chart to visualize the distribution of 'Benign' and 'Attack' labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        data = {\n",
    "            'Benign': dataset['label'].isin(BENIGN_LABEL).sum(),\n",
    "            'Malicius': dataset['label'].isin(MALICIUS_LABELS).sum()\n",
    "        }\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie(data.values(), labels=data.keys(), autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#D4FCC3'])\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(dataset)\n",
    "plot_attack_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi dei dati emergono due osservazioni principali:\n",
    "1. Il traffico normale è significativamente più rappresentato rispetto al traffico malevolo. Questo squilibrio può essere affrontato in modo intuitivo attraverso l’applicazione di tecniche di __undersampling__ sui dati relativi al traffico normale.\n",
    "2. La distribuzione dei dati degli attacchi non è uniforme.\n",
    "\n",
    "Concentrandoci sul secondo punto, l’alto sbilanciamento dei dati può influire negativamente sulle prestazioni del modello, aumentando il rischio di errori di predizione, soprattutto per le classi meno rappresentate. Per mitigare questo problema, è possibile adottare diversi approcci, ognuno con vantaggi e svantaggi:\n",
    "\n",
    "- Escludere attacchi con pochi dati\n",
    "- Applicare tecniche di undersampling e oversampling(undersampling).\n",
    "- Focalizzarsi su un singolo tipo di attacco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Per semplicità, procediamo con l'under sampling ed escludiamo il problema della distribuzione degli attacchi etichettando i dati solo con \"Benign\" e \"Malicious\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "from typing import List\n",
    "\n",
    "def clean_data(dataset: pd.DataFrame, columns_to_remove: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to clean.\n",
    "    :param List[str] columns_to_remove: The columns to remove from the dataset.\n",
    "    :return: The cleaned dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    float_cols = dataset.select_dtypes(include=['float']).columns\n",
    "    dataset[float_cols] = dataset[float_cols].round(4)\n",
    "\n",
    "    dataset = dataset.drop(columns=columns_to_remove)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "columns_to_remove = [\"flow_id\", \"src_ip\", \"dst_ip\", \"src_port\", \"timestamp\"]\n",
    "dataset = clean_data(dataset, columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def binary_labelling(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'label' column in the given dataset to binary labels.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to convert.\n",
    "    :return: The dataset with binary labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['label'] = dataset['label'].apply(lambda x: 0 if x in BENIGN_LABEL else 1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def undersampling(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Undersamples the given dataset to balance the number of benign and attack samples.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to undersample.\n",
    "    :return: The undersampled dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    benign = dataset[dataset['label'] == 0]\n",
    "    attack = dataset[dataset['label'] == 1]\n",
    "\n",
    "    benign_downsampled = resample(benign, replace=False, n_samples=len(attack), random_state=42)\n",
    "    return pd.concat([benign_downsampled, attack])\n",
    "\n",
    "\n",
    "def one_hot_encoding(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encodes the 'protocol' column in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to one-hot encode.\n",
    "    :return: The dataset with one-hot encoded 'protocol' column.\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.get_dummies(dataset, columns=['protocol'])\n",
    "\n",
    "\n",
    "def normalize_data(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the given dataset using Min-Max scaling.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to normalize.\n",
    "    :return: The normalized dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "\n",
    "dataset = binary_labelling(dataset)\n",
    "dataset = undersampling(dataset)\n",
    "dataset = one_hot_encoding(dataset)\n",
    "\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.pie(dataset['label'].value_counts(), labels=['Benign', 'Attack'], autopct='%1.1f%%', startangle=140, colors=['#D4FCC3', '#66b3ff'])\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def extract_feature_and_target(dataset: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Extracts the feature and target columns from the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to extract the feature and target columns from.\n",
    "    :return: The feature and target columns.\n",
    "    \"\"\"\n",
    "\n",
    "    return dataset.drop(columns=['label']), dataset['label']\n",
    "\n",
    "\n",
    "def select_features(dataset: pd.DataFrame, target: pd.Series, threshold: float = 0.01, xgb: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the most important features from the given dataset using an XGBoost classifier.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to select features from.\n",
    "    :param pd.Series target: The target column.\n",
    "    :param float threshold: The threshold to select features.\n",
    "    :param bool xgb: Whether to use an XGBoost classifier or RandomForest.\n",
    "    :return: The selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    if xgb:\n",
    "        clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(dataset, target)\n",
    "\n",
    "    feature_importances = pd.Series(clf.feature_importances_, index=dataset.columns).sort_values(ascending=False)\n",
    "    \n",
    "    sfm = SelectFromModel(clf, threshold=threshold, prefit=True)\n",
    "\n",
    "    return dataset.columns[sfm.get_support()]\n",
    "\n",
    "\n",
    "x, y = extract_feature_and_target(dataset)\n",
    "features = select_features(x, y)\n",
    "\n",
    "print(f\"Selected features with threshold >= 0.01:\")\n",
    "for feature in features:\n",
    "    print(feature)\n",
    "\n",
    "X = dataset[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "Procederemo con lo sviluppo del modello facendo riferimento progetto github [_\"Efficient-CNN-BiLSTMs\"_](https://github.com/jayxsinha/Efficient-CNN-BiLSTM-for-Network-IDS/) per la sua struttura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D, MaxPooling1D, BatchNormalization\n",
    "from keras.layers import Bidirectional, LSTM, Reshape, Dropout, Dense, Activation\n",
    "\n",
    "def build_model(input_shape=(122, 1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1D Convolutional layer\n",
    "    model.add(Convolution1D(64, kernel_size=122, padding=\"same\", activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    \n",
    "    # Reshape layer\n",
    "    model.add(Reshape((128, 1), input_shape=(128,)))\n",
    "    model.add(MaxPooling1D(pool_size=5))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Second Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "    \n",
    "    # Dropout layer\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Dense output layer with sigmoid activation\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape=(len(features), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_model(model, X_train, y_train, batch_size=32, epochs=10, validation_data=None):\n",
    "    \"\"\"\n",
    "    Trains the given model.\n",
    "\n",
    "    :param keras.models.Sequential model: The model to train.\n",
    "    :param np.ndarray X_train: The training features.\n",
    "    :param np.ndarray y_train: The training target.\n",
    "    :param int batch_size: The batch size for training.\n",
    "    :param int epochs: The number of epochs for training.\n",
    "    :param Tuple[np.ndarray, np.ndarray] validation_data: The validation features and target.\n",
    "    :return: The trained model and training history.\n",
    "    \"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_data\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "model, history = train_model(model, X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, X_test, y_test) -> plt.Figure:\n",
    "    y_pred_probs = model.predict(X_test).ravel()\n",
    "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    labels = [\"Benign\", \"Malicious\"]\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Original\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", color=\"darkorange\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    plot = plt.gcf()\n",
    "\n",
    "    return plot\n",
    "\n",
    "\n",
    "plot_cm_and_roc = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "def save_model(model, plot: plt.Figure, path: str = MODEL_DIR):\n",
    "    \"\"\"\n",
    "    Saves the given model to the specified path.\n",
    "\n",
    "    :param keras.models.Sequential model: The model to save.\n",
    "    :param plt.Figure plot: The plot to save.\n",
    "    :param str path: The path to save the model to.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    version = len(os.listdir(path)) + 1\n",
    "    model_dir = os.path.join(path, f\"v{version}\")\n",
    "\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "    model.save(os.path.join(model_dir, \"model.keras\"))\n",
    "    plot.savefig(os.path.join(model_dir, \"cm_and_roc.png\"))\n",
    "\n",
    "\n",
    "save_model(model, plot_cm_and_roc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
