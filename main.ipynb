{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library & Constants\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "DATASET_DIR = './data'\n",
    "\n",
    "BENIGN_LABEL = ['Benign']\n",
    "MALICIUS_LABELS = [\n",
    "    'Web_XSS', 'Heartbleed', 'Web_SQL_Injection', 'DoS_Slowhttptest',\n",
    "    'DoS_GoldenEye', 'Port_Scan', 'DDoS_LOIT', 'Botnet_ARES', 'Web_Brute_Force',\n",
    "    'SSH-Patator', 'FTP-Patator', 'DoS_Hulk', 'DoS_Slowloris'\n",
    "]\n",
    "LABELS = BENIGN_LABEL + MALICIUS_LABELS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Prima di procedere con la modellazione del dataset e alla creazione del modello, procediamo con il capire come i dati sono strutturati, cosa rappresentano e quali modifiche rapportare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "def load_data(datasets_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all datasets from a directory into a single DataFrame\n",
    "\n",
    "    :param str datasets_dir: directory containing the datasets\n",
    "    :return: pd.DataFrame containing all datasets\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            data.append(pd.read_csv(os.path.join(datasets_dir, file)))\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "\n",
    "dataset = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo l’analisi osservando la quantità di dati disponibili nel nostro dataset, BCCC-CIC-IDS2017, che comprende un totale impressionante di 2.438.052 record. Questi dati sono stati generati utilizzando NTLFlowLyzer, in contrasto con la versione precedente del dataset (CIC-IDS2017), che aveva invece adottato lo strumento CICFlowMeter per l'estrazione.\n",
    "\n",
    "Per approfondire e comprendere meglio i dati a disposizione, esploriamoli graficamente per analizzarne la distribuzione e identificarne eventuali pattern significativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "\n",
    "def plot_attack_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a bar chart to visualize the distribution of attack labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        attack_data = dataset[dataset['label'].isin(MALICIUS_LABELS)]['label'].value_counts()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(26, 10))\n",
    "        ax.bar(attack_data.index, attack_data.values, width=0.3)\n",
    "    \n",
    "\n",
    "def plot_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a pie chart to visualize the distribution of 'Benign' and 'Attack' labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        data = {\n",
    "            'Benign': dataset['label'].isin(BENIGN_LABEL).sum(),\n",
    "            'Malicius': dataset['label'].isin(MALICIUS_LABELS).sum()\n",
    "        }\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie(data.values(), labels=data.keys(), autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#D4FCC3'])\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(dataset)\n",
    "plot_attack_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi dei dati emergono due osservazioni principali:\n",
    "1. Il traffico normale è significativamente più rappresentato rispetto al traffico malevolo. Questo squilibrio può essere affrontato in modo intuitivo attraverso l’applicazione di tecniche di __undersampling__ sui dati relativi al traffico normale.\n",
    "2. La distribuzione dei dati degli attacchi non è uniforme.\n",
    "\n",
    "Concentrandoci sul secondo punto, l’alto sbilanciamento dei dati può influire negativamente sulle prestazioni del modello, aumentando il rischio di errori di predizione, soprattutto per le classi meno rappresentate. Per mitigare questo problema, è possibile adottare diversi approcci, ognuno con vantaggi e svantaggi:\n",
    "\n",
    "- Escludere attacchi con pochi dati\n",
    "- Applicare tecniche di undersampling e oversampling(undersampling).\n",
    "- Focalizzarsi su un singolo tipo di attacco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Per semplicità, procediamo con l'under sampling ed escludiamo il problema della distribuzione degli attacchi etichettando i dati solo con \"Benign\" e \"Malicious\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "from typing import List\n",
    "\n",
    "def clean_data(dataset: pd.DataFrame, columns_to_remove: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to clean.\n",
    "    :param List[str] columns_to_remove: The columns to remove from the dataset.\n",
    "    :return: The cleaned dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = dataset.dropna()\n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    float_cols = dataset.select_dtypes(include=['float']).columns\n",
    "    dataset[float_cols] = dataset[float_cols].round(4)\n",
    "\n",
    "    dataset = dataset.drop(columns=columns_to_remove)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "columns_to_remove = [\"flow_id\", \"src_ip\", \"dst_ip\", \"src_port\", \"timestamp\"]\n",
    "dataset = clean_data(dataset, columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "def binary_labelling(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'label' column in the given dataset to binary labels.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to convert.\n",
    "    :return: The dataset with binary labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['label'] = dataset['label'].apply(lambda x: 0 if x in BENIGN_LABEL else 1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def undersampling(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Undersamples the given dataset to balance the number of benign and attack samples.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to undersample.\n",
    "    :return: The undersampled dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    benign = dataset[dataset['label'] == 0]\n",
    "    attack = dataset[dataset['label'] == 1]\n",
    "\n",
    "    benign_downsampled = resample(benign, replace=False, n_samples=len(attack), random_state=42)\n",
    "    return pd.concat([benign_downsampled, attack])\n",
    "\n",
    "\n",
    "def one_hot_encoding(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-hot encodes the 'protocol' column in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to one-hot encode.\n",
    "    :return: The dataset with one-hot encoded 'protocol' column.\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.get_dummies(dataset, columns=['protocol'])\n",
    "\n",
    "\n",
    "def normalize_data(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the given dataset using Min-Max scaling.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to normalize.\n",
    "    :return: The normalized dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "\n",
    "dataset = binary_labelling(dataset)\n",
    "dataset = undersampling(dataset)\n",
    "dataset = one_hot_encoding(dataset)\n",
    "\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.pie(dataset['label'].value_counts(), labels=['Benign', 'Attack'], autopct='%1.1f%%', startangle=140, colors=['#D4FCC3', '#66b3ff'])\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def extract_feature_and_target(dataset: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Extracts the feature and target columns from the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to extract the feature and target columns from.\n",
    "    :return: The feature and target columns.\n",
    "    \"\"\"\n",
    "\n",
    "    return dataset.drop(columns=['label']), dataset['label']\n",
    "\n",
    "\n",
    "def select_features(dataset: pd.DataFrame, target: pd.Series, threshold: float = 0.01, xgb: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the most important features from the given dataset using an XGBoost classifier.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to select features from.\n",
    "    :param pd.Series target: The target column.\n",
    "    :param float threshold: The threshold to select features.\n",
    "    :param bool xgb: Whether to use an XGBoost classifier or RandomForest.\n",
    "    :return: The selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    if xgb:\n",
    "        clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(dataset, target)\n",
    "\n",
    "    feature_importances = pd.Series(clf.feature_importances_, index=dataset.columns).sort_values(ascending=False)\n",
    "    \n",
    "    sfm = SelectFromModel(clf, threshold=threshold, prefit=True)\n",
    "\n",
    "    return dataset.columns[sfm.get_support()]\n",
    "\n",
    "\n",
    "x, y = extract_feature_and_target(dataset)\n",
    "features = select_features(x, y)\n",
    "\n",
    "print(f\"Selected features with threshold >= 0.01:\")\n",
    "for feature in features:\n",
    "    print(feature)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
