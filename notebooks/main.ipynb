{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library & Constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Options\n",
    "# The following options can be modified to change the behavior of the script. These are made for the user to easily change the behavior of the script without having to modify the code.\n",
    "\n",
    "XGB_FEATER_SELECTION = True \n",
    "# If True, the script will use the feature selection method of XGBoost to select the most important features. Otherwise, the script will use the feature selection method of the Random Forest.\n",
    "\n",
    "BINARY_CLASSIFICATION = True\n",
    "# If True, the script will perform a binary classification. Otherwise, the script will perform a multiclass classification.\n",
    "\n",
    "CLASS_SELECTION = []\n",
    "# You can specify the classes you want to use for the multiclass classification. If you want to use all the classes, you can leave it empty.\n",
    "\n",
    "THRESHOLD = 0.005\n",
    "# The threshold value to be used for feature selection. The features with importance values greater than this threshold value will be selected.\n",
    "\n",
    "\n",
    "# Paths\n",
    "\n",
    "DATASET_DIR = '../data/DDOS'\n",
    "MODEL_DIR = '../models'\n",
    "\n",
    "# Datsets Info\n",
    "\n",
    "MAPPING_BINARY_LABELS = ['Benign', 'Malicious']\n",
    "BINARY_LABELS = {\n",
    "    0: 'Benign',\n",
    "    1: 'Malicious'\n",
    "}\n",
    "\n",
    "MAPPING_MULTICLASS_LABELS = {   \n",
    "    0: 'Benign',\n",
    "    1: 'Web Attack',\n",
    "    2: 'DoS Attack',\n",
    "    3: 'Network Attack',\n",
    "    4: 'Botnet',\n",
    "    5: 'Service Attack'\n",
    "}\n",
    "MULTICLASS_LABELS = {\n",
    "    0: ['Benign'],\n",
    "    1: [\"Web_XSS\", \"Web_SQL_Injection\", \"Web_Brute_Force\"],                                 # Web Attack\n",
    "    2: [\"DoS_Slowhttptest\", \"DoS_GoldenEye\", \"DDoS_LOIT\", \"DoS_Hulk\", \"DoS_Slowloris\"],     # DoS Attack\n",
    "    3: [\"Port_Scan\", \"Heartbleed\"],                                                         # Network Attack        \n",
    "    4: [\"Botnet_ARES\"],                                                                     # Botnet\n",
    "    5: [\"SSH-Patator\", \"FTP-Patator\"]                                                       # Service Attack                                           \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "def load_data(datasets_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all datasets from a directory into a single DataFrame\n",
    "\n",
    "    :param str datasets_dir: directory containing the datasets\n",
    "    :return: pd.DataFrame containing all datasets\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            data.append(pd.read_csv(os.path.join(datasets_dir, file)))\n",
    "\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = load_data(DATASET_DIR)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "\n",
    "def plot_attacks_distribution(target: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a bar chart to visualize the distribution of attack labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        attack_data = dataset[~dataset['label'].isin(MAPPING_BINARY_LABELS)]['label'].value_counts()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(26, 10))\n",
    "        ax.bar(attack_data.index, attack_data.values, width=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_distribution(dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generates a pie chart to visualize the distribution of 'Benign' and 'Attack' labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        data = {\n",
    "            'Benign': dataset['label'].isin(MAPPING_BINARY_LABELS).sum(),\n",
    "            'Malicius': dataset[~dataset['label'].isin(MAPPING_BINARY_LABELS)]['label'].value_counts().sum()\n",
    "        }\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie(data.values(), labels=data.keys(), autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#D4FCC3'])\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_distribution(dataset)\n",
    "    plot_attacks_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "from typing import List\n",
    " \n",
    "\n",
    "# def clean_data(dataset: pd.DataFrame, columns_to_remove: List[str]) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Cleans the given dataset.\n",
    "\n",
    "#     :param pd.DataFrame dataset: The dataset to clean.\n",
    "#     :param List[str] columns_to_remove: The columns to remove from the dataset.\n",
    "#     :return: The cleaned dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "#     dataset = dataset.drop_duplicates()\n",
    "#     dataset = dataset.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#     dataset = dataset.dropna()\n",
    "#     float_cols = dataset.select_dtypes(include=['float']).columns\n",
    "#     dataset[float_cols] = dataset[float_cols].round(4)\n",
    "\n",
    "#     dataset = dataset.drop(columns=columns_to_remove)\n",
    "    \n",
    "#     if len(CLASS_SELECTION) > 0:\n",
    "#         dataset = dataset[dataset['label'].isin(CLASS_SELECTION)]\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def plot_data_size_comparison(length_before: int, length_after: int):\n",
    "#     \"\"\"\n",
    "#     Plots a bar chart to compare the size of the dataset before and after cleaning.\n",
    "\n",
    "#     :param int length_before: The length of the dataset before cleaning.\n",
    "#     :param int length_after: The length of the dataset after cleaning.\n",
    "#     \"\"\"\n",
    "#     with plt.style.context('dark_background'):\n",
    "#         fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#         bars = ax.bar(['Before Cleaning', 'After Cleaning'], [length_before, length_after], color=['#66b3ff', '#ff6666'])\n",
    "#         ax.set_title('Dataset Size Before and After Cleaning')\n",
    "#         ax.set_ylabel('Number of Records')\n",
    "\n",
    "#         # Add text annotations to the bars\n",
    "#         for bar in bars:\n",
    "#             height = bar.get_height()\n",
    "#             ax.annotate(f'{height:,}',\n",
    "#                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "#                         xytext=(0, 3),  # 3 points vertical offset\n",
    "#                         textcoords=\"offset points\",\n",
    "#                         ha='center', va='bottom')\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     plt.close()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     length_before = len(dataset)\n",
    "\n",
    "#     columns_to_remove = [\"flow_id\", \"src_ip\", \"dst_ip\", \"src_port\", \"timestamp\", \"activity\"]\n",
    "#     dataset = clean_data(dataset, columns_to_remove)\n",
    "\n",
    "#     plot_data_size_comparison(length_before, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labeling\n",
    "\n",
    "def binary_labeling(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'label' column in the given dataset to binary labels.\n",
    "    0 for benign and 1 for malicious.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to convert.\n",
    "    :return: The dataset with binary labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['label'] = dataset['label'].apply(lambda x: 0 if x in MAPPING_BINARY_LABELS else 1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def multiclass_labeling(dataset: pd.DataFrame, mapping_label: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the 'label' column in the given dataset to multiclass labels.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to convert.\n",
    "    :param dict mapping_label: The mapping of labels to convert to.\n",
    "    :return: The dataset with multiclass labels.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset['label'] = dataset['label'].map(mapping_label)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if BINARY_CLASSIFICATION:\n",
    "        dataset = binary_labeling(dataset)\n",
    "    else:\n",
    "        group_map = {attack: group for group, attacks in MULTICLASS_LABELS.items() for attack in attacks}\n",
    "        dataset = multiclass_labeling(dataset, group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Features and Labels\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def extract_feature_and_target(dataset: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Extracts the feature and target columns from the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to extract the feature and target columns from.\n",
    "    :return: The feature and target columns.\n",
    "    \"\"\"\n",
    "\n",
    "    return dataset.drop(columns=['label']), dataset['label']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = extract_feature_and_target(dataset)\n",
    "\n",
    "    del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "\n",
    "def one_hot_encoding_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies one-hot encoding to all columns of type \"object\" in a pandas DataFrame.\n",
    "    \n",
    "    :param pd.DataFrame dataset: The DataFrame to apply one-hot encoding to.\n",
    "    :return: The DataFrame with one-hot encoding applied.\n",
    "    \"\"\"\n",
    "    object_columns = dataset.select_dtypes(include=['object']).columns\n",
    "    if len(object_columns) > 0:\n",
    "        dataset = pd.get_dummies(dataset, columns=object_columns, drop_first=True, dtype=np.uint8)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X = one_hot_encoding_dataset(X)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Balancing\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def plot_distribution(target: pd.Series, mapping: dict, title: str):\n",
    "    \"\"\"\n",
    "    Generates a pie chart to visualize the distribution of 'Benign' and 'Attack' labels in the given dataset.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset containing the 'label' column to generate the pie chart from.\n",
    "    :param dict mapping: The mapping of labels to convert to.\n",
    "    \"\"\"\n",
    "\n",
    "    with plt.style.context('dark_background'):\n",
    "        data = target.value_counts()\n",
    "        data.index = data.index.to_series().replace(mapping)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie(data.values, labels=data.index, autopct='%1.1f%%', startangle=140, colors=['#66b3ff', '#D4FCC3'])\n",
    "        plt.axis('equal')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def balance_data_binary_undersampler(dataset: pd.DataFrame, target: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Balances the given dataset using RandomUnderSampler.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to balance.\n",
    "    :param pd.Series target: The target column to balance.\n",
    "    :return: The balanced dataset and target.\n",
    "    \"\"\"\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_resampled, y_resampled = rus.fit_resample(dataset, target)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def balance_data_binary_cluser_based(dataset: pd.DataFrame, target: pd.Series, n_clusters: int = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Balances the dataset using clustering-based undersampling.\n",
    "    \n",
    "    :param dataset: The dataset to balance.\n",
    "    :param target: The target column to balance.\n",
    "    :param n_clusters: Number of clusters to use for undersampling. If None, it is set to the number of minority samples.\n",
    "    :return: The balanced dataset and target.\n",
    "    \"\"\"\n",
    "    majority_class = target.value_counts().idxmax()\n",
    "    minority_class = target.value_counts().idxmin()\n",
    "    \n",
    "    X_majority = dataset[target == majority_class]\n",
    "    y_majority = target[target == majority_class]\n",
    "    X_minority = dataset[target == minority_class]\n",
    "    y_minority = target[target == minority_class]\n",
    "    \n",
    "    n_clusters = n_clusters or len(X_minority)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_majority)\n",
    "    \n",
    "    X_majority_sampled = pd.DataFrame(kmeans.cluster_centers_, columns=dataset.columns)\n",
    "    y_majority_sampled = pd.Series([majority_class] * n_clusters)\n",
    "    \n",
    "    X_resampled = pd.concat([X_minority, X_majority_sampled], ignore_index=True)\n",
    "    y_resampled = pd.concat([y_minority, y_majority_sampled], ignore_index=True)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_distribution(y, MAPPING_BINARY_LABELS, 'Before Balancing')\n",
    "\n",
    "    X, y = balance_data_binary_cluser_based(X, y)\n",
    "\n",
    "    plot_distribution(y, MAPPING_BINARY_LABELS, 'After Balancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def normalize_data(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the given dataset using Min-Max scaling.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to normalize.\n",
    "    :return: The normalized dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_array = scaler.fit_transform(dataset)\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_array, columns=dataset.columns, index=dataset.index)\n",
    "    return normalized_df\n",
    "\n",
    "def standardize_data(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes the given dataset using Standard scaling.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to standardize.\n",
    "    :return: The standardized dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_array = scaler.fit_transform(dataset)\n",
    "\n",
    "    standardized_df = pd.DataFrame(standardized_array, columns=dataset.columns, index=dataset.index)\n",
    "    return standardized_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X = standardize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def select_features(dataset: pd.DataFrame, target: pd.Series, binary_classification: bool, threshold: float = 0.01, using_xgb: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects the most important features from the given dataset using a tree-based model.\n",
    "\n",
    "    :param pd.DataFrame dataset: The dataset to select features from.\n",
    "    :param pd.Series target: The target column.\n",
    "    :param bool binary_classification: Whether the task is binary classification or not.\n",
    "    :param float threshold: The threshold to use for feature selection.\n",
    "    :param bool using_xgb: Whether to use XGBoost for feature selection or not.\n",
    "    :return: The selected features.\n",
    "    \"\"\"\n",
    "\n",
    "    if using_xgb:\n",
    "        if binary_classification:\n",
    "            clf = XGBClassifier(eval_metric='logloss', random_state=42, objective='binary:logistic')\n",
    "        else:\n",
    "            clf = XGBClassifier(eval_metric='mlogloss', random_state=42, objective='multi:softmax')\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    clf.fit(dataset, target)\n",
    "    feature_importances = clf.feature_importances_\n",
    "    sfm = SelectFromModel(clf, threshold=threshold, prefit=True)\n",
    "    selected_features = dataset.columns[sfm.get_support()]\n",
    "    feature_scores = {feature: importance for feature, importance in zip(dataset.columns, feature_importances) if importance >= threshold}\n",
    "\n",
    "    return selected_features, feature_scores\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    features_name, features_scored = select_features(X, y, BINARY_CLASSIFICATION, using_xgb=XGB_FEATER_SELECTION, threshold=THRESHOLD)\n",
    "    num_features = len(features_name)\n",
    "\n",
    "    print(f\"Selected features with threshold >= {THRESHOLD} ({num_features} features):\")\n",
    "    for feature, score in sorted(features_scored.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"\\t{feature}\\t{score:.4f}\")\n",
    "\n",
    "    X = X[features_name]\n",
    "    print(\"Dataset after Feature Selection:\")\n",
    "    print(X.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "Procederemo con lo sviluppo del modello facendo riferimento progetto github [_\"Efficient-CNN-BiLSTMs\"_](https://github.com/jayxsinha/Efficient-CNN-BiLSTM-for-Network-IDS/) per la sua struttura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# The neural network model works only wiht numpy arrays, so we need to convert the dataframes to numpy arrays \n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Moreover, the neural network model has a specific input shape, so we need to reshape the data\n",
    "X_train = X_train.reshape((-1, num_features, 1))\n",
    "X_test = X_test.reshape((-1, num_features, 1))\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Desing\n",
    "\n",
    "I layer di questo modello sono i seguenti:\n",
    "- CNN: Utile per apprendere le relazioni piÃ¹ nascoste tra le feature\n",
    "- 2 LSTMs: Per poter capire come le feature hanno effetto su lungo raggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Convolution1D, MaxPooling1D, BatchNormalization, Bidirectional, LSTM, \n",
    "    Dropout, Dense, Activation, RepeatVector)\n",
    "\n",
    "def build_model(input_shape=(122, 1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1D Convolutional layer\n",
    "    model.add(Convolution1D(64, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # RepeatVector to adapt dimensions\n",
    "    model.add(RepeatVector(1))  # Ripete il vettore per adattarlo al prossimo LSTM\n",
    "    \n",
    "    # Second Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # Dense output layer with sigmoid activation\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(input_shape=(num_features, 1 ))\n",
    "\n",
    "    print(\"Model Summary:\")\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_model(model, X_train, y_train, batch_size=32, epochs=10, patience=3, validation_data=None):\n",
    "    \"\"\"\n",
    "    Trains the given model with early stopping.\n",
    "\n",
    "    :param keras.models.Sequential model: The model to train.\n",
    "    :param np.ndarray X_train: The training features.\n",
    "    :param np.ndarray y_train: The training target.\n",
    "    :param int batch_size: The batch size for training.\n",
    "    :param int epochs: The number of epochs for training.\n",
    "    :param Tuple[np.ndarray, np.ndarray] validation_data: The validation features and target.\n",
    "    :return: The trained model and training history.\n",
    "    \"\"\"\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model, history = train_model(model, X_train, y_train, epochs=100, patience=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Evaluation\n",
    "\n",
    "def plot_training_metrics(history) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics from the history object.\n",
    "\n",
    "    :param history: The training history object.\n",
    "    :return: Matplotlib figure of the training metrics.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    train_acc = history.history.get('accuracy', None)\n",
    "    val_acc = history.history.get('val_accuracy', None)\n",
    "    if train_acc and val_acc:\n",
    "        ax[0].plot(train_acc, label='Training Accuracy', marker='o')\n",
    "        ax[0].plot(val_acc, label='Validation Accuracy', marker='x')\n",
    "        ax[0].set_title('Training and Validation Accuracy per Epoch')\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Accuracy')\n",
    "        ax[0].legend()\n",
    "        ax[0].grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history.get('val_loss', None)\n",
    "    ax[1].plot(train_loss, label='Training Loss', marker='o')\n",
    "    if val_loss:\n",
    "        ax[1].plot(val_loss, label='Validation Loss', marker='x')\n",
    "    ax[1].set_title('Training and Validation Loss per Epoch')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_history = plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_evaluation_metrics(y_test, y_pred, y_pred_probs, is_binary=True, target_mapping=None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix and ROC curve for the given predictions.\n",
    "\n",
    "    :param np.ndarray y_test: The true target labels.\n",
    "    :param np.ndarray y_pred: The predicted target labels.\n",
    "    :param np.ndarray y_pred_probs: The predicted target probabilities.\n",
    "    :param bool is_binary: Whether the classification is binary or multiclass.\n",
    "    :param dict target_mapping: The mapping of target labels.\n",
    "    :return: Matplotlib figure of the evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_binary:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    if target_mapping:\n",
    "        labels = [target_mapping.get(label, label) for label in np.unique(y_test)]\n",
    "    else:\n",
    "        labels = np.unique(y_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=labels, yticklabels=labels, \n",
    "                ax=axes[0] if is_binary else ax)\n",
    "    \n",
    "    if is_binary:\n",
    "        axes[0].set_xlabel(\"Predicted\")\n",
    "        axes[0].set_ylabel(\"Original\")\n",
    "        axes[0].set_title(\"Confusion Matrix\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"Original\")\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "    if is_binary:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        axes[1].plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\", color=\"darkorange\")\n",
    "        axes[1].plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
    "        axes[1].set_xlabel(\"False Positive Rate\")\n",
    "        axes[1].set_ylabel(\"True Positive Rate\")\n",
    "        axes[1].set_title(\"Receiver Operating Characteristic\")\n",
    "        axes[1].legend(loc=\"lower right\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, is_binary=True, target_mapping=None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Evaluates the given model using the test data and plots the evaluation metrics.\n",
    "\n",
    "    :param keras.models.Sequential model: The trained model to evaluate.\n",
    "    :param np.ndarray X_test: The test features.\n",
    "    :param np.ndarray y_test: The test target.\n",
    "    :param bool is_binary: Whether the classification is binary or multiclass.\n",
    "    :param dict target_mapping: The mapping of target labels.\n",
    "    :return: Matplotlib figure of the evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if is_binary:\n",
    "        y_pred_probs = model.predict(X_test).ravel()\n",
    "        y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_pred_probs = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1:.2f}\")\n",
    "\n",
    "    return plot_evaluation_metrics(y_test, y_pred, y_pred_probs, is_binary, target_mapping)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_cm_and_roc = evaluate_model(model, X_test, y_test, is_binary=BINARY_CLASSIFICATION, target_mapping=BINARY_LABELS if BINARY_CLASSIFICATION else MAPPING_MULTICLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "import json\n",
    "\n",
    "def save_model(model, features: list, mapping_used: list, plots: dict, path: str = \"../models\"):\n",
    "    \"\"\"\n",
    "    Saves the given model, features, and plots to the specified path.\n",
    "\n",
    "    :param keras.models.Sequential model: The model to save.\n",
    "    :param list features: The list of features used by the model.\n",
    "    :param dict plots: A dictionary of plots to save.\n",
    "    :param str path: The path to save the model to.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    version = len(os.listdir(path)) + 1\n",
    "    model_dir = os.path.join(path, f\"v{version}\")\n",
    "\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(os.path.join(model_dir, \"model.keras\"))\n",
    "\n",
    "    # Save the features as a JSON file\n",
    "    with open(os.path.join(model_dir, \"features.json\"), \"w\") as f:\n",
    "        json.dump(features, f, indent=4)\n",
    "\n",
    "    # Save the mapping used as a JSON file\n",
    "    with open(os.path.join(model_dir, \"mapping.json\"), \"w\") as f:\n",
    "        json.dump(mapping_used, f, indent=4)\n",
    "\n",
    "    # Save the plots\n",
    "    for name, plot in plots.items():\n",
    "        plot.savefig(os.path.join(model_dir, f\"{name}.png\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    figure_dict = {\n",
    "        \"plot_cm_and_roc\": plot_cm_and_roc,\n",
    "        \"plot_history\": plot_history    \n",
    "    }\n",
    "\n",
    "    save_model(model, features_name, BINARY_LABELS if BINARY_CLASSIFICATION else MAPPING_MULTICLASS_LABELS, figure_dict, MODEL_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
